{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.conda/envs/hypermoon/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# transformer block 1\n",
    "# overlapping patch merging -> [selfefficient attention -> Mix-FFN]\n",
    "#The OverlapPatchMerging block can be implemented with a convolution layer with a stride less than the kernel_size, so it overlaps different patches. In SegFormer, the conv layer is followed by a layer norm.\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from timm.models.layers import DropPath, to_2tuple, trunc_normal_, to_3tuple\n",
    "import math\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "#from mmcv.runner import load_checkpoint\n",
    "#from mmseg.utils import get_root_logger\n",
    "\n",
    "class OverlapPatchEmbed(nn.Module):\n",
    "    \"\"\" Image to Patch Embedding\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, img_size=224, patch_size=7, stride=4, in_chans=1, embed_dim=768):\n",
    "        super().__init__()\n",
    "        if isinstance(img_size, int) :\n",
    "            img_size = to_3tuple(img_size)\n",
    "        if isinstance(patch_size, int) :\n",
    "            patch_size = to_3tuple(patch_size)\n",
    "        #img_size = to_3tuple(img_size)\n",
    "        #patch_size = to_3tuple(patch_size)\n",
    "\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.D, self.H, self.W = img_size[0] // patch_size[0], img_size[1] // patch_size[1], img_size[2]//patch_size[2]\n",
    "        self.num_patches = self.D * self.H * self.W\n",
    "        self.proj = nn.Conv3d(in_chans, embed_dim, kernel_size=patch_size, stride=stride,\n",
    "                              padding=(patch_size[0] // 2, patch_size[1] // 2, patch_size[2] // 2))\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "        elif isinstance(m, nn.Conv3d):\n",
    "            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.kernel_size[2] * m.out_channels\n",
    "            fan_out //= m.groups\n",
    "            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n",
    "            if m.bias is not None:\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)\n",
    "        _, _, D, H, W = x.shape\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        x = self.norm(x)\n",
    "\n",
    "        return x, D, H, W\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U openmim\n",
    "# !mim install mmcv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor = torch.randn(1, 2, 3, 4, 5)\n",
    "# torch.randn(1, 1, 224, 224, 224).shape\n",
    "# model = OverlapPatchEmbed()\n",
    "# print(model(torch.randn(1, 1, 224, 224, 224))[0].shape)\n",
    "# #embed_tensor = model(tensor)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(tensor.permute(0, 2, 3, 4, 1).shape)\n",
    "# print(tensor.reshape(1, 3, -1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(tensor.reshape(1, -1, 3).shape)\n",
    "# print(tensor.transpose(-2, -1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m = nn.Softmax(dim=-1)\n",
    "# input = torch.randn(2, 3)\n",
    "# print(input)\n",
    "# output = m(input)\n",
    "# print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(np.exp(input[1]))\n",
    "# exp = np.exp(input[1])\n",
    "# exp = np.array(exp)\n",
    "# print(type(exp))\n",
    "# print(np.sum(exp))\n",
    "# exp = exp/np.sum(exp)\n",
    "# print(exp)\n",
    "# #print(np.sum(np.exp(input[0])))\n",
    "# #norm = np.exp(input[0])/np.sum(np.exp(input[0]))\n",
    "# #print(norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0., sr_ratio=1):\n",
    "        super().__init__()\n",
    "        assert dim % num_heads == 0, f\"dim {dim} should be divided by num_heads {num_heads}.\"\n",
    "\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "\n",
    "        self.q = nn.Linear(dim, dim, bias=qkv_bias)\n",
    "        self.kv = nn.Linear(dim, dim * 2, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "        self.sr_ratio = sr_ratio\n",
    "        if sr_ratio > 1:\n",
    "            self.sr = nn.Conv3d(dim, dim, kernel_size=sr_ratio, stride=sr_ratio)\n",
    "            self.norm = nn.LayerNorm(dim)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "        elif isinstance(m, nn.Conv3d):\n",
    "            fan_out = m.kernel_size[0] * m.kernel_size[1] *  m.kernel_size[2] * m.out_channels\n",
    "            fan_out //= m.groups\n",
    "            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n",
    "            if m.bias is not None:\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x, D, H, W ):\n",
    "        B, N, C = x.shape\n",
    "        q = self.q(x).reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n",
    "\n",
    "        if self.sr_ratio > 1:\n",
    "            x_ = x.permute(0, 2, 1).reshape(B, C, D, H, W)\n",
    "            x_ = self.sr(x_).reshape(B, C, -1).permute(0, 2, 1)\n",
    "            x_ = self.norm(x_)\n",
    "            kv = self.kv(x_).reshape(B, -1, 2, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        else:\n",
    "            kv = self.kv(x).reshape(B, -1, 2, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        k, v = kv[0], kv[1]\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "        \n",
    "\n",
    "        #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DWConv(nn.Module):\n",
    "    def __init__(self, dim=768):\n",
    "        super(DWConv, self).__init__()\n",
    "        self.dwconv = nn.Conv3d(dim, dim, 3, 1, 1, bias=True, groups=dim)\n",
    "\n",
    "    def forward(self, x, D, H, W):\n",
    "        B, N, C = x.shape\n",
    "        x = x.transpose(1, 2).view(B, C, D, H, W)\n",
    "        x = self.dwconv(x)\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mlp(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.dwconv = DWConv(hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "        elif isinstance(m, nn.Conv3d):\n",
    "            fan_out = m.kernel_size[0] * m.kernel_size[1] *  m.kernel_size[2] * m.out_channels\n",
    "            fan_out //= m.groups\n",
    "            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n",
    "            if m.bias is not None:\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x, D, H, W):\n",
    "        x = self.fc1(x)\n",
    "        x = self.dwconv(x, D, H, W)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 27, 768])\n",
      "1 27 768\n",
      "torch.Size([1, 768, 27])\n",
      "torch.Size([1, 768, 3, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.randn(1, 27, 768)\n",
    "model = DWConv(dim=768)\n",
    "print(model.forward(tensor, 3, 3, 3).size())\n",
    "#print(model(tensor)) \n",
    "\n",
    "B, N, C = tensor.shape\n",
    "print(B, N, C)\n",
    "tensor = tensor.transpose(1, 2)\n",
    "print(tensor.size())\n",
    "\n",
    "tensor = tensor.transpose(1, 2).view(B, C, 3, 3, 3)\n",
    "print(tensor.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21504"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 768 \n",
    "q = nn.Linear(dim, dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(224, 224)\n"
     ]
    }
   ],
   "source": [
    "t = torch.randn(1, 3, 224, 224)\n",
    "t.flatten(2).transpose(1, 2).shape\n",
    "tup =to_2tuple(224)\n",
    "print(tup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 50176])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.flatten(2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "scale = None or 0.5\n",
    "print(scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
    "                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm, sr_ratio=1):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = Attention(\n",
    "            dim,\n",
    "            num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "            attn_drop=attn_drop, proj_drop=drop, sr_ratio=sr_ratio)\n",
    "        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "        elif isinstance(m, nn.Conv3d):\n",
    "            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "            fan_out //= m.groups\n",
    "            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n",
    "            if m.bias is not None:\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x, D, H, W):\n",
    "        x = x + self.drop_path(self.attn(self.norm1(x), D, H, W))\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x), D, H, W))\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop_path_rate = 0\n",
    "# depths=[3, 4, 6, 3]\n",
    "# print(sum(depths))\n",
    "# lst = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]\n",
    "# print(lst)\n",
    "# print(len(lst))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for x in torch.linspace(0, 12, sum(depths)):\n",
    "#     print(x.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixVisionTransformer(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=1, num_classes=1000, embed_dims=[64, 128, 256, 512],\n",
    "                 num_heads=[1, 2, 4, 8], mlp_ratios=[4, 4, 4, 4], qkv_bias=False, qk_scale=None, drop_rate=0.,\n",
    "                 attn_drop_rate=0., drop_path_rate=0., norm_layer=nn.LayerNorm,\n",
    "                 depths=[3, 4, 6, 3], sr_ratios=[8, 4, 2, 1]):\n",
    "        super().__init__()\n",
    "        assert isinstance(img_size, int) or (isinstance(img_size, tuple) and len(img_size) == 3), f\"the image {img_size} is the type of {type(img_size)}. It has to be int or tuple of 3 int\"\n",
    "        self.num_classes = num_classes\n",
    "        self.depths = depths\n",
    "\n",
    "        # patch_embed\n",
    "        if isinstance(img_size, int):\n",
    "            self.patch_embed1 = OverlapPatchEmbed(img_size=img_size, patch_size=7, stride=4, in_chans=in_chans,\n",
    "                                              embed_dim=embed_dims[0])\n",
    "            self.patch_embed2 = OverlapPatchEmbed(img_size=img_size // 4, patch_size=3, stride=2, in_chans=embed_dims[0],\n",
    "                                              embed_dim=embed_dims[1])\n",
    "            self.patch_embed3 = OverlapPatchEmbed(img_size=img_size // 8, patch_size=3, stride=2, in_chans=embed_dims[1],\n",
    "                                              embed_dim=embed_dims[2])\n",
    "            self.patch_embed4 = OverlapPatchEmbed(img_size=img_size // 16, patch_size=3, stride=2, in_chans=embed_dims[2],\n",
    "                                              embed_dim=embed_dims[3])\n",
    "            \n",
    "        elif isinstance(img_size, tuple):\n",
    "            self.patch_embed1 = OverlapPatchEmbed(img_size=(img_size[0], img_size[1], img_size[2]), patch_size=7, stride=4, in_chans=in_chans,\n",
    "                                              embed_dim=embed_dims[0])\n",
    "            self.patch_embed2 = OverlapPatchEmbed(img_size=(img_size[0] // 4, img_size[1] //4, img_size[2]//4), patch_size=3, stride=2, in_chans=embed_dims[0],\n",
    "                                              embed_dim=embed_dims[1])\n",
    "            self.patch_embed3 = OverlapPatchEmbed(img_size=(img_size[0] // 8, img_size[1]//8, img_size[2]//8), patch_size=3, stride=2, in_chans=embed_dims[1],\n",
    "                                              embed_dim=embed_dims[2])\n",
    "            self.patch_embed4 = OverlapPatchEmbed(img_size=(img_size[0] // 16, img_size[1]//16, img_size[2]//16), patch_size=3, stride=2, in_chans=embed_dims[2],\n",
    "                                              embed_dim=embed_dims[3])\n",
    "\n",
    "        # transformer encoder\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]  # stochastic depth decay rule\n",
    "        cur = 0\n",
    "        self.block1 = nn.ModuleList([Block(\n",
    "            dim=embed_dims[0], num_heads=num_heads[0], mlp_ratio=mlp_ratios[0], qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "            drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[cur + i], norm_layer=norm_layer,\n",
    "            sr_ratio=sr_ratios[0])\n",
    "            for i in range(depths[0])])\n",
    "        self.norm1 = norm_layer(embed_dims[0])\n",
    "\n",
    "        cur += depths[0]\n",
    "        self.block2 = nn.ModuleList([Block(\n",
    "            dim=embed_dims[1], num_heads=num_heads[1], mlp_ratio=mlp_ratios[1], qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "            drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[cur + i], norm_layer=norm_layer,\n",
    "            sr_ratio=sr_ratios[1])\n",
    "            for i in range(depths[1])])\n",
    "        self.norm2 = norm_layer(embed_dims[1])\n",
    "\n",
    "        cur += depths[1]\n",
    "        self.block3 = nn.ModuleList([Block(\n",
    "            dim=embed_dims[2], num_heads=num_heads[2], mlp_ratio=mlp_ratios[2], qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "            drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[cur + i], norm_layer=norm_layer,\n",
    "            sr_ratio=sr_ratios[2])\n",
    "            for i in range(depths[2])])\n",
    "        self.norm3 = norm_layer(embed_dims[2])\n",
    "\n",
    "        cur += depths[2]\n",
    "        self.block4 = nn.ModuleList([Block(\n",
    "            dim=embed_dims[3], num_heads=num_heads[3], mlp_ratio=mlp_ratios[3], qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "            drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[cur + i], norm_layer=norm_layer,\n",
    "            sr_ratio=sr_ratios[3])\n",
    "            for i in range(depths[3])])\n",
    "        self.norm4 = norm_layer(embed_dims[3])\n",
    "\n",
    "        # classification head\n",
    "        # self.head = nn.Linear(embed_dims[3], num_classes) if num_classes > 0 else nn.Identity()\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "        elif isinstance(m, nn.Conv3d):\n",
    "            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "            fan_out //= m.groups\n",
    "            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n",
    "            if m.bias is not None:\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    # def init_weights(self, pretrained=None):\n",
    "    #     if isinstance(pretrained, str):\n",
    "    #         logger = get_root_logger()\n",
    "    #         load_checkpoint(self, pretrained, map_location='cpu', strict=False, logger=logger)\n",
    "\n",
    "    def reset_drop_path(self, drop_path_rate):\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(self.depths))]\n",
    "        cur = 0\n",
    "        for i in range(self.depths[0]):\n",
    "            self.block1[i].drop_path.drop_prob = dpr[cur + i]\n",
    "\n",
    "        cur += self.depths[0]\n",
    "        for i in range(self.depths[1]):\n",
    "            self.block2[i].drop_path.drop_prob = dpr[cur + i]\n",
    "\n",
    "        cur += self.depths[1]\n",
    "        for i in range(self.depths[2]):\n",
    "            self.block3[i].drop_path.drop_prob = dpr[cur + i]\n",
    "\n",
    "        cur += self.depths[2]\n",
    "        for i in range(self.depths[3]):\n",
    "            self.block4[i].drop_path.drop_prob = dpr[cur + i]\n",
    "\n",
    "    def freeze_patch_emb(self):\n",
    "        self.patch_embed1.requires_grad = False\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {'pos_embed1', 'pos_embed2', 'pos_embed3', 'pos_embed4', 'cls_token'}  # has pos_embed may be better\n",
    "\n",
    "    def get_classifier(self):\n",
    "        return self.head\n",
    "\n",
    "    def reset_classifier(self, num_classes, global_pool=''):\n",
    "        self.num_classes = num_classes\n",
    "        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        B = x.shape[0]\n",
    "        outs = []\n",
    "\n",
    "        # stage 1\n",
    "        x, D, H, W = self.patch_embed1(x)\n",
    "        for i, blk in enumerate(self.block1):\n",
    "            x = blk(x, D, H, W)\n",
    "        x = self.norm1(x)\n",
    "        x = x.reshape(B, D, H, W, -1).permute(0, 4, 1, 2, 3).contiguous()\n",
    "        outs.append(x)\n",
    "\n",
    "        # stage 2\n",
    "        x, D, H, W = self.patch_embed2(x)\n",
    "        for i, blk in enumerate(self.block2):\n",
    "            x = blk(x, D, H, W)\n",
    "        x = self.norm2(x)\n",
    "        x = x.reshape(B, D, H, W, -1).permute(0, 4, 1, 2, 3).contiguous()\n",
    "        outs.append(x)\n",
    "\n",
    "        # stage 3\n",
    "        x, D, H, W = self.patch_embed3(x)\n",
    "        for i, blk in enumerate(self.block3):\n",
    "            x = blk(x, D, H, W)\n",
    "        x = self.norm3(x)\n",
    "        x = x.reshape(B, D, H, W, -1).permute(0, 4, 1, 2, 3).contiguous()\n",
    "        outs.append(x)\n",
    "\n",
    "        # stage 4\n",
    "        x, D, H, W = self.patch_embed4(x)\n",
    "        for i, blk in enumerate(self.block4):\n",
    "            x = blk(x, D, H, W)\n",
    "        x = self.norm4(x)\n",
    "        x = x.reshape(B, D, H, W, -1).permute(0, 4, 1, 2, 3).contiguous()\n",
    "        outs.append(x)\n",
    "\n",
    "        return outs\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.forward_features(x)\n",
    "        # x = self.head(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = MixVisionTransformer()\n",
    "# #print(model)\n",
    "# tensor = torch.randn(1, 1, 224, 224, 224)\n",
    "# print(tensor.shape)\n",
    "# print(model(tensor)[0].shape, model(tensor)[1].shape, model(tensor)[2].shape, model(tensor)[3].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DWConv(\n",
      "  (dwconv): Conv3d(768, 768, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=768)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pyproject-toml\n",
    "#!pip install wheel\n",
    "#!pip install functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class mit_b1(MixVisionTransformer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(mit_b1, self).__init__(\n",
    "            patch_size=4, embed_dims=[64, 128, 320, 512], num_heads=[1, 2, 5, 8], mlp_ratios=[4, 4, 4, 4],\n",
    "            qkv_bias=True, norm_layer=nn.LayerNorm, depths=[2, 2, 2, 2], sr_ratios=[8, 4, 2, 1],\n",
    "            drop_rate=0.0, drop_path_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = mit_b1()\n",
    "# #print(model)\n",
    "# tensor = torch.randn(1, 1, 224, 224, 224)\n",
    "# print(tensor.shape)\n",
    "# print(model(tensor)[0].shape, model(tensor)[1].shape, model(tensor)[2].shape, model(tensor)[3].shape)\n",
    "# output_tensor = model(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DWConv(\n",
      "  (dwconv): Conv3d(768, 768, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=768)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Linear Embedding\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=2048, embed_dim=768):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(input_dim, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        x = self.proj(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegFormerHead(nn.Module):\n",
    "    \"\"\"\n",
    "    SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers\n",
    "    \"\"\"\n",
    "    def __init__(self, feature_strides, in_channels=[64, 128, 320, 512] ,embedding_dim = 768, num_classes=19, dropout_ratio=0.1):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        assert len(feature_strides) == len(self.in_channels)\n",
    "        assert min(feature_strides) == feature_strides[0]\n",
    "        self.feature_strides = feature_strides\n",
    "        self.num_classes = num_classes\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        self.dropout = nn.Dropout(self.dropout_ratio)\n",
    "\n",
    "\n",
    "        c1_in_channels, c2_in_channels, c3_in_channels, c4_in_channels = self.in_channels\n",
    "\n",
    "        self.linear_c4 = MLP(input_dim=c4_in_channels, embed_dim=embedding_dim)\n",
    "        self.linear_c3 = MLP(input_dim=c3_in_channels, embed_dim=embedding_dim)\n",
    "        self.linear_c2 = MLP(input_dim=c2_in_channels, embed_dim=embedding_dim)\n",
    "        self.linear_c1 = MLP(input_dim=c1_in_channels, embed_dim=embedding_dim)\n",
    "\n",
    "        self.linear_fuse = nn.Sequential(\n",
    "            nn.Conv3d(embedding_dim*4, embedding_dim, kernel_size=1, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm3d(embedding_dim)\n",
    "        )\n",
    "\n",
    "        self.linear_pred = nn.Conv3d(embedding_dim, self.num_classes, kernel_size=1)\n",
    "\n",
    "    def resize(self, input, size=None, scale_factor=None, mode='nearest', align_corners=None):\n",
    "            return F.interpolate(input, size, scale_factor, mode, align_corners)\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        c1, c2, c3, c4 = x\n",
    "\n",
    "        ############## MLP decoder on C1-C4 ###########\n",
    "        n, _, d, h, w = c4.shape\n",
    "\n",
    "        _c4 = self.linear_c4(c4).permute(0,2,1).reshape(n, -1, c4.shape[2], c4.shape[3], c4.shape[4])\n",
    "        _c4 = self.resize(_c4, size=c1.size()[2:],mode='trilinear',align_corners=False)\n",
    "\n",
    "        _c3 = self.linear_c3(c3).permute(0,2,1).reshape(n, -1, c3.shape[2], c3.shape[3], c3.shape[4])\n",
    "        _c3 = self.resize(_c3, size=c1.size()[2:],mode='trilinear',align_corners=False)\n",
    "\n",
    "        _c2 = self.linear_c2(c2).permute(0,2,1).reshape(n, -1, c2.shape[2], c2.shape[3], c2.shape[4])\n",
    "        _c2 = self.resize(_c2, size=c1.size()[2:],mode='trilinear',align_corners=False)\n",
    "\n",
    "        _c1 = self.linear_c1(c1).permute(0,2,1).reshape(n, -1, c1.shape[2], c1.shape[3], c1.shape[4])\n",
    "\n",
    "        _c = self.linear_fuse(torch.cat([_c4, _c3, _c2, _c1], dim=1))\n",
    "\n",
    "        x = self.dropout(_c)\n",
    "        x = self.linear_pred(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #print(type(model(tensor)))\n",
    "# inputs = model(tensor)\n",
    "# print(len(inputs))\n",
    "# c1, c2, c3, c4 = inputs\n",
    "# print(c1.shape, c2.shape, c3.shape, c4.shape)\n",
    "# n, _, d, h, w = c4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def resize(input, size=None, scale_factor=None, mode='nearest', align_corners=None):\n",
    "#     return F.interpolate(input, size, scale_factor, mode, align_corners)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear = MLP(input_dim=512, embed_dim=768)\n",
    "# #print(linear(c4).permute(0,2,1).reshape(n, -1, c4.shape[2], c4.shape[3], c4.shape[4]).shape)\n",
    "# _c4 = linear(c4).permute(0,2,1).reshape(n, -1, c4.shape[2], c4.shape[3], c4.shape[4]) \n",
    "# print(_c4.shape)\n",
    "# _c4 = resize(_c4, size=c1.size()[2:],mode='trilinear',align_corners=False)\n",
    "# print(_c4.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = SegFormerHead(feature_strides=[4, 8, 16, 32], in_channels=[64, 128, 320, 512] ,embedding_dim = 768, num_classes=19)\n",
    "# #print(model)\n",
    "# model(output_tensor).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThreeDimSegFormer(nn.Module):\n",
    "    def __init__(self, img_size=(200,224,224), patch_size=4, in_chans=1, num_classes=1000, embed_dims=[64, 128, 256, 512],\n",
    "                 num_heads=[1, 2, 4, 8], mlp_ratios=[4, 4, 4, 4], qkv_bias=True, qk_scale=None, drop_rate=0.,\n",
    "                 attn_drop_rate=0., drop_path_rate=0.1, norm_layer=nn.LayerNorm,\n",
    "                 depths=[3, 4, 6, 3], sr_ratios=[8, 4, 2, 1]):\n",
    "        super().__init__()\n",
    "        self.encoder = MixVisionTransformer(img_size=img_size, patch_size=patch_size, in_chans=in_chans, num_classes=num_classes, embed_dims=embed_dims,\n",
    "                 num_heads=num_heads, mlp_ratios=mlp_ratios, qkv_bias=qkv_bias, qk_scale=qk_scale, drop_rate=drop_rate,\n",
    "                 attn_drop_rate=attn_drop_rate, drop_path_rate=drop_path_rate, norm_layer=norm_layer,\n",
    "                 depths=depths, sr_ratios=sr_ratios)\n",
    "        self.decoder = SegFormerHead(feature_strides=[4, 8, 16, 32], in_channels=embed_dims ,embedding_dim = embed_dims[-1], num_classes=num_classes)\n",
    "    def final_resize(self, input, size=None, scale_factor=None, mode='nearest', align_corners=None):\n",
    "        return F.interpolate(input, size, scale_factor, mode, align_corners)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        x = self.encoder(input)\n",
    "        x = self.decoder(x)\n",
    "        out = self.final_resize(x, size=input.shape[2:],mode='trilinear',align_corners=False)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ThreeDimSegFormer(\n",
      "  (encoder): MixVisionTransformer(\n",
      "    (patch_embed1): OverlapPatchEmbed(\n",
      "      (proj): Conv3d(1, 64, kernel_size=(7, 7, 7), stride=(4, 4, 4), padding=(3, 3, 3))\n",
      "      (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (patch_embed2): OverlapPatchEmbed(\n",
      "      (proj): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
      "      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (patch_embed3): OverlapPatchEmbed(\n",
      "      (proj): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
      "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (patch_embed4): OverlapPatchEmbed(\n",
      "      (proj): Conv3d(256, 512, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
      "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (block1): ModuleList(\n",
      "      (0): Block(\n",
      "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (q): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (kv): Linear(in_features=64, out_features=128, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          (sr): Conv3d(64, 64, kernel_size=(8, 8, 8), stride=(8, 8, 8))\n",
      "          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
      "          (dwconv): DWConv(\n",
      "            (dwconv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=256)\n",
      "          )\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (1): Block(\n",
      "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (q): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (kv): Linear(in_features=64, out_features=128, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          (sr): Conv3d(64, 64, kernel_size=(8, 8, 8), stride=(8, 8, 8))\n",
      "          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (drop_path): DropPath(drop_prob=0.014)\n",
      "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
      "          (dwconv): DWConv(\n",
      "            (dwconv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=256)\n",
      "          )\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "    (block2): ModuleList(\n",
      "      (0): Block(\n",
      "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (q): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (kv): Linear(in_features=128, out_features=256, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          (sr): Conv3d(128, 128, kernel_size=(4, 4, 4), stride=(4, 4, 4))\n",
      "          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (drop_path): DropPath(drop_prob=0.029)\n",
      "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (dwconv): DWConv(\n",
      "            (dwconv): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=512)\n",
      "          )\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (1): Block(\n",
      "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (q): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (kv): Linear(in_features=128, out_features=256, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          (sr): Conv3d(128, 128, kernel_size=(4, 4, 4), stride=(4, 4, 4))\n",
      "          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (drop_path): DropPath(drop_prob=0.043)\n",
      "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (dwconv): DWConv(\n",
      "            (dwconv): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=512)\n",
      "          )\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    (block3): ModuleList(\n",
      "      (0): Block(\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (kv): Linear(in_features=256, out_features=512, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          (sr): Conv3d(256, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
      "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (drop_path): DropPath(drop_prob=0.057)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          (dwconv): DWConv(\n",
      "            (dwconv): Conv3d(1024, 1024, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=1024)\n",
      "          )\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (1): Block(\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (kv): Linear(in_features=256, out_features=512, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          (sr): Conv3d(256, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
      "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (drop_path): DropPath(drop_prob=0.071)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          (dwconv): DWConv(\n",
      "            (dwconv): Conv3d(1024, 1024, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=1024)\n",
      "          )\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (block4): ModuleList(\n",
      "      (0): Block(\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (q): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (kv): Linear(in_features=512, out_features=1024, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (drop_path): DropPath(drop_prob=0.086)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (dwconv): DWConv(\n",
      "            (dwconv): Conv3d(2048, 2048, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=2048)\n",
      "          )\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (1): Block(\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (q): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (kv): Linear(in_features=512, out_features=1024, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (drop_path): DropPath(drop_prob=0.100)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (dwconv): DWConv(\n",
      "            (dwconv): Conv3d(2048, 2048, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=2048)\n",
      "          )\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm4): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (decoder): SegFormerHead(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (linear_c4): MLP(\n",
      "      (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    )\n",
      "    (linear_c3): MLP(\n",
      "      (proj): Linear(in_features=256, out_features=512, bias=True)\n",
      "    )\n",
      "    (linear_c2): MLP(\n",
      "      (proj): Linear(in_features=128, out_features=512, bias=True)\n",
      "    )\n",
      "    (linear_c1): MLP(\n",
      "      (proj): Linear(in_features=64, out_features=512, bias=True)\n",
      "    )\n",
      "    (linear_fuse): Sequential(\n",
      "      (0): Conv3d(2048, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "      (1): ReLU()\n",
      "      (2): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (linear_pred): Conv3d(512, 1000, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "  )\n",
      ")\n",
      "torch.Size([1, 1, 100, 124, 124])\n",
      "torch.Size([1, 1000, 100, 124, 124])\n"
     ]
    }
   ],
   "source": [
    "model = ThreeDimSegFormer(depths=[2, 2, 2, 2])\n",
    "print(model)\n",
    "device = torch.device('cuda:0')\n",
    "model = model.to(device)\n",
    "tensor = torch.randn(1, 1, 100, 124, 124)\n",
    "tensor = tensor.to(device)\n",
    "print(tensor.shape)\n",
    "print(model(tensor).shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hypermoon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
